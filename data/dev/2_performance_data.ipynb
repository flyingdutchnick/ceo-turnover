{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a dataset where each observation has a company, a date, that company's trailing stock returns and volatility over different lengths, as well as that information for the market benchmark (SP500). To do so, we will use a Compustat dataset of monthly stock prices for companies on American exchanges and a Yahoo Finance dataset of SP500 monthly prices.\n",
    "\n",
    "__________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We begin by importing IntegrateData subclasses responsible for creating and merging the datasets. \n",
    "\n",
    "IntegrateData is an abstract superclass which all integrators inherit from. It defines methods for reading and writing csv data either locally or on S3, as well as maintaining a copy of such data in a pandas dataframe. Subclasses must define the integrate_data() method, which executes the necessary manipulations to the internally stored data such that it is ready for output. For example, the IntegrateStockData subclass' integrate_data() method uses price information in its internal df to calculate and write new columns with hold historical stock returns.\n",
    "\n",
    "The only IntegrateData subclass method that is publically called is process(), which runs the sequence of reading, processing, cleaning, and writing the data as specified by the constructor and the subclass' concrete implementation of integrate_data()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from integrate_stock import IntegrateStockData\n",
    "from volatility_data import IntegrateVolatilityData\n",
    "from sp500_data import IntegrateSPData\n",
    "from sp500_data_merge import MergeSPWithMaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dataset to construct will have stock returns and volatility. We import a file named 'na-monthly-share-price.csv', currently stored in S3. The IntegrateStockData object will calculate returns based on prices, add that data to the existing data, and write that to the location specified by stock_returns_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Integrating 3 month trailing returns: 100%|██████████| 2749846/2749846 [03:56<00:00, 11609.01it/s]\n",
      "Integrating 6 month trailing returns: 100%|██████████| 2749846/2749846 [03:55<00:00, 11660.57it/s]\n",
      "Integrating 12 month trailing returns: 100%|██████████| 2749846/2749846 [03:56<00:00, 11640.53it/s]\n",
      "Integrating 24 month trailing returns: 100%|██████████| 2749846/2749846 [03:55<00:00, 11686.84it/s]\n",
      "Integrating 36 month trailing returns: 100%|██████████| 2749846/2749846 [03:55<00:00, 11691.60it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_price_path = 'na-monthly-share-price.csv'\n",
    "stock_returns_path = 'na-monthly-share-returns.csv'\n",
    "\n",
    "integrate_price = IntegrateStockData(stock_price_path, stock_returns_path, input_type='s3')\n",
    "integrate_price.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay.. 20 minutes isn't too bad for 5 returns periods each with 2.7 million observations.\n",
    "\n",
    "Next, we add volatility data to the mix. This is done through an IntegrateVolatilityData object, which uses the pandas pd.DataFrame.std() method to calculate standard deviations of trailing returns. Given that each data point now requires accessing and performing operations on an array of data points, rather than two singletons, this takes a bit (lot) longer than calculating returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Integrating 3 month historical volatility: 100%|██████████| 2749846/2749846 [15:34<00:00, 2942.99it/s]\n",
      "Integrating 6 month historical volatility: 100%|██████████| 2749846/2749846 [15:17<00:00, 2995.66it/s]\n",
      "Integrating 12 month historical volatility: 100%|██████████| 2749846/2749846 [14:23<00:00, 3183.93it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_returns_vol_path = 'na-monthly-share-returns.csv'\n",
    "\n",
    "integrate_volatility = IntegrateVolatilityData(stock_returns_path, stock_returns_vol_path)\n",
    "integrate_volatility.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "45 minutes.. just enough time to debug another DateTime error. Our stock dataset now has all of the necessary individual performance data, so it's time to create the SP500 performance dataset. The IntegrateSPData class' integrate_data() method essentially replicates the functionality of the IntegrateStockData and IntegrateVolatilityData classes, adjusted for the Yahoo Finance dataset formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Integrating SP500 1 month returns: 100%|██████████| 243/243 [00:00<00:00, 42081.58it/s]\n",
      "Integrating SP500 3 month returns: 100%|██████████| 243/243 [00:00<00:00, 37396.93it/s]\n",
      "Integrating SP500 6 month returns: 100%|██████████| 243/243 [00:00<00:00, 35298.74it/s]\n",
      "Integrating SP500 12 month returns: 100%|██████████| 243/243 [00:00<00:00, 43157.85it/s]\n",
      "Integrating SP500 24 month returns: 100%|██████████| 243/243 [00:00<00:00, 45236.16it/s]\n",
      "Integrating SP500 36 month returns: 100%|██████████| 243/243 [00:00<00:00, 47870.74it/s]\n",
      "Integrating SP500 3 month historical volatility: 100%|██████████| 243/243 [00:00<00:00, 2939.51it/s]\n",
      "Integrating SP500 6 month historical volatility: 100%|██████████| 243/243 [00:00<00:00, 3074.55it/s]\n",
      "Integrating SP500 12 month historical volatility: 100%|██████████| 243/243 [00:00<00:00, 3087.05it/s]\n"
     ]
    }
   ],
   "source": [
    "sp_500_price_path = 'sp500-monthly-price.csv'\n",
    "sp_500_returns_vol_path = 'sp500-returns-volatility.csv'\n",
    "\n",
    "construct_sp_data = IntegrateSPData(sp_500_price_path, sp_500_returns_vol_path, input_type='s3')\n",
    "construct_sp_data.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, that one was fast. The only thing left to do is merge the two datasets! The MergeSPWithMaster class seems to be a good pick for that. It merges by matching the dates in the stock and SP500 datasets, so that each observation for a stock has added market benchmark information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging stock performance and market performance datasets: 100%|██████████| 42/42 [00:09<00:00,  4.57it/s]\n"
     ]
    }
   ],
   "source": [
    "master_data_path = '1_performance_data.csv'\n",
    "\n",
    "merge_data = MergeSPWithMaster(stock_returns_vol_path, sp_500_returns_vol_path, master_data_path, output_type='s3')\n",
    "merge_data.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data has now been merged and written to S3! All in a day's work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
